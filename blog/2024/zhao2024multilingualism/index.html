<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <div style="text-align: center;"> <img src="/assets/papers/zhao2024multilingualism/header.png" alt="paper_header"> </div> <p>In this work, the authors introduce a new framework for explaining the property of multilingualism in Multilingual Large Language Models (MLLMs). How to define this property of ‘Multlingualism’? The property of multilingualism enables MLLMs to text in multiple languages. In this paper, the authors have introduced a framework they call as <strong>MWork</strong> which states that MLLMs first convert queries to a unified representation, then they reason on this unified representation together with multlingual knowledge extraction and finally they translate queries to the target language.</p> <ol> <li> <p>Conversion to a <em>unified representation</em> - <br> The authors state that the first step in handling non-English queries is them being converted to English in the initial layers of MLLMs. From Hou et al., 2023, attention mechanisms are utilized in reasoning over a corpus of text. The authors observe a decrease in the number of <em>non-English</em> neurons (?) of the attention matrices in the middle layers of the MLLMs. <em>Can reasoning be performed over text present in a language other than English?</em> It is unlikely owing to the decline in the number of non-English neurons in the reasoning (attentions structures) regions of MLLMs.</p> </li> <li> <p>Task-Solving - <br> <em>What about the neurons in the FFN layers?</em> - The authors observe NO decrease in the number of language specific neurons in FFN structures across all layers of MLLMs. Hence, the authors decompose the task-solving into two steps : <em>thinking</em> in English and <em>extracting knowledge</em> multilingually from language-specific neurons in the middle layers of MLLMs.</p> </li> <li> <p>Generation of output - <br>- <strong>MWork</strong> proposes that models <em>generate</em> responses by translating back to the query’s original lanuage. <em>At this stage, one can think of a simple verifyiing experiment for this step</em>. Suppose we have query in two non-English language say, French and Urdu. Consider a query requiring extraction of a knowledge point prevantely common in both French and Urdu literature. <em>Will there be a inconsitence in MLLM performance?</em> (Maybe we can intercept the residual embeddings to see the checkpoint of back-translation initiation for languages structurally dissimlar to English.)</p> </li> </ol> <p><strong>How will I verify it?</strong></p> <p>I will look at each of the three steps separately : <em>conversion</em> to unified English representation, <em>task-solving</em> and <em>translation</em>. <strong><em>Conversion</em></strong> -&gt; Relative amount of English / non-English tokens; <em>Does the converted representation still relate to the original query? (Can we quantify it?) &lt;- credits to Eshaan Tanwar from LCS2 for suggesting this.</em> 2. <strong><em>Task-Solving</em> MLWork</strong> claims that knowledge is extracted from language specific neurons in the FFN structures. <em>Does deactivating these neurons have an affect on the performance?</em> ALSO, is the knowledge diffused into different languages or is localized to the language of the pretraining text? <em>Consider a setup when some knowledge is only limited to Vietnamese literature. Does deactivating neurons of FFN pertaining to Veitnamese completely downgrade MLLM performance?</em><br><br><br> Is there a way to identify neurons relating to some knowledge?<br> How much overlap do they have with language specific neurons?<br><br><br> <em><strong>Translation</strong></em> -&gt; I will certainly want to look at works trying to mechanistically interpret translation in MLLMs. (Is there a gap here?). Using those, one can design experiments to test this. <br> The authors found that “<em>deactivating randomly sampled neurons in the task-solving layers disables the capabilities of LLMs in reasoning to a greater extent than deactivating randomly sampled neurons in all layers</em>”. This proves that the middle layers are associated with <em>thinking</em> + <em>extracting knowledge</em> (basically <em>task-solving</em>). (Note the authors refer to these <em>middle</em> layers of MLLMs as the <em>task-solving</em> layer.) <br><br> <strong>How to verify separation of <em>thinking in English</em> and <em>extracting knowledge multilingually?</em></strong> <br><br></p> <ul> <li>I would have created a synthetic task that does not involve any nuanced knowledge extraction (<em>Commonsense reasoning</em> over multiple languages). Taking some non-English languages under study : <strong>(1)</strong> Deactivate these language-specific neurons in the FFN structures of the <em>task-solving</em> layer. (Since, I am not sure if knowledge from one language gets leaked into other language-specific neurons of FFN, let me propose all of these language-specific neurons of FFN) <strong>(2)</strong> Compare performance of MLLMs with and without deactivation. Moreover, deactivating language-specific neurons of the attention structures in the <em>task-solving</em> layer should not have much effect on non-English queries as compared to English queries.</li> <li>The authors also claim that <em>deactivating language-specific neurons within the feed-forward structure of the task-solving layer predominantly affects non-English languages. This implies that processing multilingual queries necessitates accessing the multilingual information embedded within the relevant structures.</em> <br> <strong>How to verify the <em>translation</em>/<em>generation</em> structure of MWork</strong> The authors have deactivated language-specific neurons in the final layers of the MLLMs and commented on a decline in performance. As I mentioned earlier, there can be ways to selectively comment on the nature of translation happening at this stage. <br> The authors have further shown that selectively fine-tuning the parameters in the language-specific neurons enhances the performance of MLLMs for queries in that language. <br> <strong>Questions for further research -</strong> </li> <li>Polyglots are shown to process native languages at much greater ease as compared to other languages, even at the same level of proficiency. Given we declare as the native language for the LLMs, can we quantify if being subjective to pre-training in multilingual corpora, affect the <em>ease</em> of handling English queries. (Let the setting be purely reasoning based and let <em>ease</em> be quantified as the number of neurons activated in the task-solving layer.</li> </ul> </body></html>