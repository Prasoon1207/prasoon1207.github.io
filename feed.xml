<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://prasoon1207.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://prasoon1207.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-09T18:09:55+00:00</updated><id>https://prasoon1207.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">llmsociety</title><link href="https://prasoon1207.github.io/blog/2024/prasoon2024llmsociety/" rel="alternate" type="text/html" title="llmsociety"/><published>2024-10-05T15:09:00+00:00</published><updated>2024-10-05T15:09:00+00:00</updated><id>https://prasoon1207.github.io/blog/2024/prasoon2024llmsociety</id><content type="html" xml:base="https://prasoon1207.github.io/blog/2024/prasoon2024llmsociety/"><![CDATA[<div style="text-align: center;"> <img src="/assets/img/llmsociety/marvin_minsky.jpg" alt="Marvin Minsky"/> </div> <p><br/></p> <p>I often consider myself lucky (and blessed) to have gotten to approach computational language modelling and its subsidiaries after taking all relevant linguisitcs courses offered by the Humanities Department at IIT Delhi. I remember myself getting deeply intrigued about the impact of language understanding in shaping the intellect of new-born babies and how human intelligence (which manifests to intelligence and theorems) got accelerated by collaboration enabled using complex language symbols. When I met people who explained to me first, the task of <strong>next word prediction</strong> using deep learning, I found myself comfortably positioned to harness my knowledge and deep internal ambition to <strong>modelling human intelligence</strong>. This blog is just the projection of my initial thoughts on this idea and I have gathered the required confidence to translate my ambition to a workable project after going through papers listed in the <em>References</em> subsection below.</p> <p><br/></p> <div style="text-align: center;"> <img src="/assets/img/llmsociety/psychmodelhumanintel.png" alt="a psychological model of human intelligence" width="500" height="300"/> </div> <p><br/></p> <p>In our efforts to model human intelligence, we should take a great deal of motivation from the foundational psychological discourses (like <em>Society Of Minds</em> from Marvin Minsky). My personal recipe of convenient computational modelling of human intelligence assumes that all human intelligence is basically a community of human minds, that itself is composed of like-minded/ group experts of human beings. There are channels of communication between these sub-communities of human minds (or society of agents as I like to call them). There are also ways in which these agents communicate with each other, they discuss a domain-specific problem (<em>let there be interpretability!</em>), reach a consensus and forward their decision/reasoning to other communities.</p> <p><br/> <strong>Where does LLM come in?</strong></p> <div style="text-align: center;"> <img src="/assets/img/llmsociety/deepmodelhumanintel.png" alt="a deep net model of human intelligence" width="500" height="300"/> </div> <p><br/> Replacing these individual human minds / agents with LLMs seems the way to go. Thinking from first principles, a human mind is a more general computational unit. Similarly, this model is not focussing on creating <em>expert LLMs</em> but sort of <em>expert LLM communities</em>. <br/></p> <p><strong>What are some open problems that I would like to tackle?</strong></p> <ul> <li>Profiling of LLMs/ Communities of LLMs: Is it possible to create an automatic metric to measure human-like characteristics of LLMs, such as ‘overconfident’, ‘humour’ etc.</li> <li>Dynamic Communication: Most of the literature today, focussing on multi-model multi-agent collaborations, uses hard prompts to make LLMs communicate with each other. Can we use a specialized <strong>clan community</strong> of models to create soft prompts that enable better communication, if any.</li> </ul> <p><strong>References</strong></p> <ul> <li>ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs</li> <li>MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning</li> <li>Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View</li> <li>Society of Minds</li> </ul>]]></content><author><name></name></author><category term="opinion"/><summary type="html"><![CDATA[Modelling human intelligence with a community of LLMs.]]></summary></entry><entry><title type="html">llm2vec</title><link href="https://prasoon1207.github.io/blog/2024/parishad2024llm2vec/" rel="alternate" type="text/html" title="llm2vec"/><published>2024-09-24T15:09:00+00:00</published><updated>2024-09-24T15:09:00+00:00</updated><id>https://prasoon1207.github.io/blog/2024/parishad2024llm2vec</id><content type="html" xml:base="https://prasoon1207.github.io/blog/2024/parishad2024llm2vec/"><![CDATA[<div style="text-align: center;"> <img src="/assets/papers/parishad2024llm2vec/header.png" alt="paper_header"/> </div> <p><br/><br/></p> <p><strong>Idea</strong></p> <p>Excellent ideas can come from the smallest of observations. We know that there is a widespread use of <strong>bidirectional encoders</strong> for producing rich text representations, both word-level and sentence-level. This ability is instilled in them by their pretraining recipe, which enables them to 1) predict the <MASK> token by looking at the left and right context around it and, 2) predict how likely two sentences could appear together, in a context. The pretraining recipe is designed in a way that it sort of _mask's_ a significant chunk of words (~15%). However training **unidirectional encoders** involves them having complete visibility over the pretraining data. It only makes sense to compare their performance as produces of contextual embeddings. We will now take a brief look at the framework called <a href="https://github.com/McGill-NLP/llm2vec">**llm2vec**</a>, which enables backwards-looking decoders to create even better text representations than their bidirectional counterparts.</MASK></p> <p><br/> <br/></p> <p><strong>Making future visible</strong> <br/></p> <p>To create good word-level representations, it is imperative to look at both left-sided and right-sided contexts. The authors propose replacing the causal attention matrix with a matrix of only 1’s. This ensures bidirectional interactions of token representations and leads to more <em>context-aware</em> embeddings. A point that was impressively acknowledged in the paper, was that since these decoders are trained in one direction, it is not obvious whether the embeddings produced by imposing bidirectionality, have any rich meaning. This is also observed by them experimentally. It is commendable that the authors push through this, what could have been a sad evening at the research lab and introduce two more crucial steps in the llm2vec framework.</p> <p><strong>What is the next <em>masked</em> word?</strong> <br/> Given a sequence of tokens, $(w_{1}, w_{2}, [MASK], w_{3}, w_{4}, …)$, bidirectional encoders create a representation of $[MASK]$ from the lateral context. To instil creating rich representations using decoder-based LLMs, the authors propose <strong>MNTP</strong> (<strong>M</strong>asked <strong>N</strong>ext <strong>T</strong>oken <strong>P</strong>rediction). The model now uses the representation of $w_{2}$ (in the bidirectional sense) to check for similarity against the golden representation of the true $[MASK]$ token.</p> <p><strong>Are these two sentences related?</strong> <br/> Given a collection of sentences <strong>S</strong>, for any two sentences, say $s_{i}$ and $s_{j}$ (in <strong>S</strong>), we know if these sentences could appear <strong>meaningfully</strong> in a context. The authors use this technique, called as <strong>unsupervised contrastive learning</strong>, to make better sentence representations. The representations are created by pooling the constituent word representations.</p>]]></content><author><name></name></author><category term="papers"/><category term="paper"/><summary type="html"><![CDATA[Can decoders produce rich representations?]]></summary></entry><entry><title type="html">Future</title><link href="https://prasoon1207.github.io/blog/2024/Future/" rel="alternate" type="text/html" title="Future"/><published>2024-09-02T15:09:00+00:00</published><updated>2024-09-02T15:09:00+00:00</updated><id>https://prasoon1207.github.io/blog/2024/Future</id><content type="html" xml:base="https://prasoon1207.github.io/blog/2024/Future/"><![CDATA[<h3 id="hi">Hi</h3> <p>I will be adding more such discussions/ my takes on papers I like.<br/></p> <p>Hope you are doing well.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How I feel about this.]]></summary></entry><entry><title type="html">MWork</title><link href="https://prasoon1207.github.io/blog/2024/zhao2024multilingualism/" rel="alternate" type="text/html" title="MWork"/><published>2024-08-31T15:09:00+00:00</published><updated>2024-08-31T15:09:00+00:00</updated><id>https://prasoon1207.github.io/blog/2024/zhao2024multilingualism</id><content type="html" xml:base="https://prasoon1207.github.io/blog/2024/zhao2024multilingualism/"><![CDATA[<div style="text-align: center;"> <img src="/assets/papers/zhao2024multilingualism/header.png" alt="paper_header"/> </div> <p><br/><br/></p> <p>In this work, the authors introduce a new framework for explaining the property of multilingualism in Multilingual Large Language Models (MLLMs). How do we define this property of ‘Multilingualism’? The property of multilingualism enables MLLMs to text in multiple languages. In this paper, the authors have introduced a framework they call <strong>MWork</strong> which states that MLLMs first convert queries to a unified representation. They reason on this unified representation with multilingual knowledge extraction and finally, they translate queries to the target language.</p> <ol> <li> <p>Conversion to a <em>unified representation</em> - <br/> The authors state that the first step in handling non-English queries is to convert them to English in the initial layers of MLLMs. According to Hou et al., 2023, attention mechanisms are utilized in reasoning over a text corpus. The authors observe a decrease in the number of <em>non-English</em> neurons (?) of the attention matrices in the middle layers of the MLLMs. <em>Can reasoning be performed over text in a language other than English?</em> It is unlikely owing to the decline in the number of non-English neurons in the reasoning (attention structures) regions of MLLMs.</p> </li> <li> <p>Task-Solving - <br/> <em>What about the neurons in the FFN layers?</em> - The authors observe NO decrease in the number of language-specific neurons in FFN structures across all layers of MLLMs. Hence, the authors decompose the task-solving into two steps: <em>thinking</em> in English and <em>extracting knowledge</em> multilingually from language-specific neurons in the middle layers of MLLMs.</p> </li> <li> <p>Generation of output - <br/>- <strong>MWork</strong> proposes that models <em>generate</em> responses by translating back to the query’s original language. <em>At this stage, one can think of a simple verifying experiment for this step</em>. Suppose we have a query in two non-English languages say, French and Urdu. Consider a query requiring extraction of a knowledge point prevalently common in both French and Urdu literature. <em>Will there be an inconsistency in MLLM performance?</em> (Maybe we can intercept the residual embeddings to see the checkpoint of back-translation initiation for languages structurally dissimilar to English.)</p> </li> </ol> <p><strong>How will I verify it?</strong></p> <p>I will look at each of the three steps separately: <em>conversion</em> to unified English representation, <em>task-solving</em> and <em>translation</em>. <strong><em>Conversion</em></strong> -&gt; Relative amount of English / non-English tokens; <em>Does the converted representation still relate to the original query? (Can we quantify it?) &lt;- Credits to Eshaan Tanwar from LCS2 for suggesting this.</em> 2. <strong><em>Task-Solving</em> MLWork</strong> claims that knowledge is extracted from language-specific neurons in the FFN structures. <em>Does deactivating these neurons affect the performance?</em> ALSO, is the knowledge diffused into different languages or localized to the language of the pretraining text? <em>Consider a setup when some knowledge is only limited to Vietnamese literature. Does deactivating neurons of FFN of Vietnamese completely downgrade MLLM performance?</em><br/><br/><br/> Is there a way to identify neurons relating to some knowledge?<br/> How much overlap do they have with language-specific neurons?<br/><br/><br/> <em><strong>Translation</strong></em> -&gt; I will certainly want to look at works trying to mechanistically interpret translation in MLLMs. (Is there a gap here?). Using those, one can design experiments to test this. <br/> The authors found that “<em>deactivating randomly sampled neurons in the task-solving layers disables the capabilities of LLMs in reasoning to a greater extent than deactivating randomly sampled neurons in all layers</em>”. This proves that the middle layers are associated with <em>thinking</em> + <em>extracting knowledge</em> (basically <em>task-solving</em>). (Note the authors refer to these <em>middle</em> layers of MLLMs as the <em>task-solving</em> layer.) <br/><br/> <strong>How to verify separation of <em>thinking in English</em> and <em>extracting knowledge multilingually?</em></strong> <br/><br/></p> <ul> <li>I would have created a synthetic task that does not involve any nuanced knowledge extraction (<em>Commonsense reasoning</em> over multiple languages). Taking some non-English languages under study : <strong>(1)</strong> Deactivate these language-specific neurons in the FFN structures of the <em>task-solving</em> layer. (Since I am not sure if knowledge from one language gets leaked into other language-specific neurons of FFN, let me propose all of these language-specific neurons of FFN) <strong>(2)</strong> Compare the performance of MLLMs with and without deactivation. Moreover, deactivating language-specific neurons of the attention structures in the <em>task-solving</em> layer should not have much effect on non-English queries as compared to English queries.</li> <li>The authors also claim that <em>deactivating language-specific neurons within the feed-forward structure of the task-solving layer predominantly affects non-English languages</em>. This implies that processing multilingual queries necessitates accessing the multilingual information embedded within the relevant structures.<br/><br/></li> </ul> <p><strong>How to verify the translation/generation structure of MWork</strong>- The authors have deactivated language-specific neurons in the final layers of the MLLMs and commented on a decline in performance. As I mentioned earlier, there can be ways to selectively comment on the nature of translation happening at this stage. The authors have further shown that selectively fine-tuning the parameters in the language-specific neurons enhances the performance of MLLMs for queries in that language. <br/> <strong>Questions for further research -</strong></p> <ul> <li>Polyglots are shown to process native languages at much greater ease as compared to other languages, even at the same level of proficiency. Given we declare it as the native language for the LLMs, can we quantify if being subjective to pre-training in multilingual corpora, affects the <em>ease</em> of handling English queries? (Let the setting be purely reasoning-based and let <em>ease</em> be quantified as the number of neurons activated in the task-solving layer.</li> </ul>]]></content><author><name></name></author><category term="papers"/><category term="paper"/><category term="multilingualism"/><summary type="html"><![CDATA[How do Large Language Models Handle Multilingualism?]]></summary></entry></feed>