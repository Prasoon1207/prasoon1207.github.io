<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://prasoon1207.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://prasoon1207.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-05T00:19:59+00:00</updated><id>https://prasoon1207.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Rl0</title><link href="https://prasoon1207.github.io/blog/2024/RL0/" rel="alternate" type="text/html" title="Rl0"/><published>2024-09-04T00:00:00+00:00</published><updated>2024-09-04T00:00:00+00:00</updated><id>https://prasoon1207.github.io/blog/2024/RL0</id><content type="html" xml:base="https://prasoon1207.github.io/blog/2024/RL0/"><![CDATA[<h3 id="notes-on-reinforcement-learning">Notes on Reinforcement Learning.</h3> <p>I will be writing some notes for the learning paradigm where you reinforce rewarding behaviour into an agent.<br/> Hope this helps someone getting introduced to RL (and me too).<br/></p> <h3 id="motivation-">Motivation :</h3> <p>Imagine you are an early man wandering alone deep inside an unknown terrain. You encounter a forest with many brightly coloured berries vastly spread across the landscape. This environment is completely new to you and unfortunately, you are starving. You taste a red berry and it causes a skin irritation to you. However, it does not diminish your hunger and you move on to taste a blue berry. You find it tasty but you are distracted by many orange berries around them. You taste one of those and it hurts your stomach. You go on to try many differently coloured berries and resolve in your mind that berries which are red(~<em>ish</em>) are bad and blue(~<em>ish</em>) berries are good. You now communicate this <em>policy</em> to you community and thanks to you, I am now enjoying a blue-berry yoghurt right now while writing this post.<br/> <em>What was this paradigm of learning?</em> Did the early man find physiological differences in the berries just based on their colour?<br/> Did the early man hear this pattern of effect from other people? <br/></p> <p>This paradigm of learning is called reinforcement learning. The umbrella for making an <em>agent</em> (early man) learn a <em>policy</em> (blue berries are good while red berries are bad) from some <em>rewards</em> ( upsetting stomach and skin irritation) obtained by interacting with an <em>environment</em>(whole observable and tangible forest landscape). <br/></p> <p>Let us now formalize this. <br/> (<em>Note this lecture is titled <strong>RLO</strong> because it serves as a birdseye view for the entire RL landscape</em>)</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Notes on Reinforcement Learning.]]></summary></entry><entry><title type="html">MLWork</title><link href="https://prasoon1207.github.io/blog/2024/zhao2024multilingualism/" rel="alternate" type="text/html" title="MLWork"/><published>2024-08-31T15:09:00+00:00</published><updated>2024-08-31T15:09:00+00:00</updated><id>https://prasoon1207.github.io/blog/2024/zhao2024multilingualism</id><content type="html" xml:base="https://prasoon1207.github.io/blog/2024/zhao2024multilingualism/"><![CDATA[<div style="text-align: center;"> <img src="/assets/papers/zhao2024multilingualism/header.png" alt="paper_header"/> </div> <p>In this work, the authors introduce a new framework for explaining the property of multilingualism in Multilingual Large Language Models (MLLMs). How to define this property of ‘Multlingualism’? The property of multilingualism enables MLLMs to text in multiple languages. In this paper, the authors have introduced a framework they call as <strong>MWork</strong> which states that MLLMs first convert queries to a unified representation, then they reason on this unified representation together with multlingual knowledge extraction and finally they translate queries to the target language.</p> <ol> <li> <p>Conversion to a <em>unified representation</em> - <br/> The authors state that the first step in handling non-English queries is them being converted to English in the initial layers of MLLMs. From Hou et al., 2023, attention mechanisms are utilized in reasoning over a corpus of text. The authors observe a decrease in the number of <em>non-English</em> neurons (?) of the attention matrices in the middle layers of the MLLMs. <em>Can reasoning be performed over text present in a language other than English?</em> It is unlikely owing to the decline in the number of non-English neurons in the reasoning (attentions structures) regions of MLLMs.</p> </li> <li> <p>Task-Solving - <br/> <em>What about the neurons in the FFN layers?</em> - The authors observe NO decrease in the number of language specific neurons in FFN structures across all layers of MLLMs. Hence, the authors decompose the task-solving into two steps : <em>thinking</em> in English and <em>extracting knowledge</em> multilingually from language-specific neurons in the middle layers of MLLMs.</p> </li> <li> <p>Generation of output - <br/>- <strong>MWork</strong> proposes that models <em>generate</em> responses by translating back to the query’s original lanuage. <em>At this stage, one can think of a simple verifyiing experiment for this step</em>. Suppose we have query in two non-English language say, French and Urdu. Consider a query requiring extraction of a knowledge point prevantely common in both French and Urdu literature. <em>Will there be a inconsitence in MLLM performance?</em> (Maybe we can intercept the residual embeddings to see the checkpoint of back-translation initiation for languages structurally dissimlar to English.)</p> </li> </ol> <p><strong>How will I verify it?</strong></p> <p>I will look at each of the three steps separately : <em>conversion</em> to unified English representation, <em>task-solving</em> and <em>translation</em>. <strong><em>Conversion</em></strong> -&gt; Relative amount of English / non-English tokens; <em>Does the converted representation still relate to the original query? (Can we quantify it?) &lt;- credits to Eshaan Tanwar from LCS2 for suggesting this.</em> 2. <strong><em>Task-Solving</em> MLWork</strong> claims that knowledge is extracted from language specific neurons in the FFN structures. <em>Does deactivating these neurons have an affect on the performance?</em> ALSO, is the knowledge diffused into different languages or is localized to the language of the pretraining text? <em>Consider a setup when some knowledge is only limited to Vietnamese literature. Does deactivating neurons of FFN pertaining to Veitnamese completely downgrade MLLM performance?</em><br/><br/><br/> Is there a way to identify neurons relating to some knowledge?<br/> How much overlap do they have with language specific neurons?<br/><br/><br/> <em><strong>Translation</strong></em> -&gt; I will certainly want to look at works trying to mechanistically interpret translation in MLLMs. (Is there a gap here?). Using those, one can design experiments to test this. <br/> The authors found that “<em>deactivating randomly sampled neurons in the task-solving layers disables the capabilities of LLMs in reasoning to a greater extent than deactivating randomly sampled neurons in all layers</em>”. This proves that the middle layers are associated with <em>thinking</em> + <em>extracting knowledge</em> (basically <em>task-solving</em>). (Note the authors refer to these <em>middle</em> layers of MLLMs as the <em>task-solving</em> layer.) <br/><br/> <strong>How to verify separation of <em>thinking in English</em> and <em>extracting knowledge multilingually?</em></strong> <br/><br/></p> <ul> <li>I would have created a synthetic task that does not involve any nuanced knowledge extraction (<em>Commonsense reasoning</em> over multiple languages). Taking some non-English languages under study : <strong>(1)</strong> Deactivate these language-specific neurons in the FFN structures of the <em>task-solving</em> layer. (Since, I am not sure if knowledge from one language gets leaked into other language-specific neurons of FFN, let me propose all of these language-specific neurons of FFN) <strong>(2)</strong> Compare performance of MLLMs with and without deactivation. Moreover, deactivating language-specific neurons of the attention structures in the <em>task-solving</em> layer should not have much effect on non-English queries as compared to English queries.</li> <li>The authors also claim that <em>deactivating language-specific neurons within the feed-forward structure of the task-solving layer predominantly affects non-English languages. This implies that processing multilingual queries necessitates accessing the multilingual information embedded within the relevant structures.</em> <br/> <strong>How to verify the <em>translation</em>/<em>generation</em> structure of MWork</strong> The authors have deactivated language-specific neurons in the final layers of the MLLMs and commented on a decline in performance. As I mentioned earlier, there can be ways to selectively comment on the nature of translation happening at this stage. <br/> The authors have further shown that selectively fine-tuning the parameters in the language-specific neurons enhances the performance of MLLMs for queries in that language. <br/> <strong>Questions for further research -</strong></li> <li>Polyglots are shown to process native languages at much greater ease as compared to other languages, even at the same level of proficiency. Given we declare as the native language for the LLMs, can we quantify if being subjective to pre-training in multilingual corpora, affect the <em>ease</em> of handling English queries. (Let the setting be purely reasoning based and let <em>ease</em> be quantified as the number of neurons activated in the task-solving layer.</li> </ul>]]></content><author><name></name></author><category term="papers"/><category term="paper"/><category term="multilingualism"/><summary type="html"><![CDATA[How do Large Language Models Handle Multilingualism?]]></summary></entry><entry><title type="html">a post with videos</title><link href="https://prasoon1207.github.io/blog/2023/videos/" rel="alternate" type="text/html" title="a post with videos"/><published>2023-04-24T21:01:00+00:00</published><updated>2023-04-24T21:01:00+00:00</updated><id>https://prasoon1207.github.io/blog/2023/videos</id><content type="html" xml:base="https://prasoon1207.github.io/blog/2023/videos/"><![CDATA[<p>This is an example post with videos. It supports local video files.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all. </div> <p>It does also support embedding videos from different sources. Here are some examples:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/jNQXAC9IVRw" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://player.vimeo.com/video/524933864?h=1ac4fd9fb4&amp;title=0&amp;byline=0&amp;portrait=0" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="videos"/><summary type="html"><![CDATA[this is what included videos could look like]]></summary></entry><entry><title type="html">a post with math</title><link href="https://prasoon1207.github.io/blog/2015/math/" rel="alternate" type="text/html" title="a post with math"/><published>2015-10-20T15:12:00+00:00</published><updated>2015-10-20T15:12:00+00:00</updated><id>https://prasoon1207.github.io/blog/2015/math</id><content type="html" xml:base="https://prasoon1207.github.io/blog/2015/math/"><![CDATA[<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\sum_{k=1}^\infty |\langle x, e_k \rangle|^2 \leq \|x\|^2\] <p>You can also use <code class="language-plaintext highlighter-rouge">\begin{equation}...\end{equation}</code> instead of <code class="language-plaintext highlighter-rouge">$$</code> for display mode math. MathJax will automatically number equations:</p> <p>\begin{equation} \label{eq:cauchy-schwarz} \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) \end{equation}</p> <p>and by adding <code class="language-plaintext highlighter-rouge">\label{...}</code> inside the equation environment, we can now refer to the equation using <code class="language-plaintext highlighter-rouge">\eqref</code>.</p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[an example of a blog post with some math]]></summary></entry></feed>