<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://prasoon1207.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://prasoon1207.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-13T18:59:45+00:00</updated><id>https://prasoon1207.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Future</title><link href="https://prasoon1207.github.io/blog/2024/Future/" rel="alternate" type="text/html" title="Future"/><published>2024-09-02T15:09:00+00:00</published><updated>2024-09-02T15:09:00+00:00</updated><id>https://prasoon1207.github.io/blog/2024/Future</id><content type="html" xml:base="https://prasoon1207.github.io/blog/2024/Future/"><![CDATA[<h3 id="hi">Hi</h3> <p>I will be adding more such discussions/ my takes on papers I like.<br/></p> <p>Hope you are doing well.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How I feel about this.]]></summary></entry><entry><title type="html">MWork</title><link href="https://prasoon1207.github.io/blog/2024/zhao2024multilingualism/" rel="alternate" type="text/html" title="MWork"/><published>2024-08-31T15:09:00+00:00</published><updated>2024-08-31T15:09:00+00:00</updated><id>https://prasoon1207.github.io/blog/2024/zhao2024multilingualism</id><content type="html" xml:base="https://prasoon1207.github.io/blog/2024/zhao2024multilingualism/"><![CDATA[<div style="text-align: center;"> <img src="/assets/papers/zhao2024multilingualism/header.png" alt="paper_header"/> </div> <p>In this work, the authors introduce a new framework for explaining the property of multilingualism in Multilingual Large Language Models (MLLMs). How do we define this property of ‘Multilingualism’? The property of multilingualism enables MLLMs to text in multiple languages. In this paper, the authors have introduced a framework they call <strong>MWork</strong> which states that MLLMs first convert queries to a unified representation. They reason on this unified representation with multilingual knowledge extraction and finally, they translate queries to the target language.</p> <ol> <li> <p>Conversion to a <em>unified representation</em> - <br/> The authors state that the first step in handling non-English queries is to convert them to English in the initial layers of MLLMs. According to Hou et al., 2023, attention mechanisms are utilized in reasoning over a text corpus. The authors observe a decrease in the number of <em>non-English</em> neurons (?) of the attention matrices in the middle layers of the MLLMs. <em>Can reasoning be performed over text in a language other than English?</em> It is unlikely owing to the decline in the number of non-English neurons in the reasoning (attention structures) regions of MLLMs.</p> </li> <li> <p>Task-Solving - <br/> <em>What about the neurons in the FFN layers?</em> - The authors observe NO decrease in the number of language-specific neurons in FFN structures across all layers of MLLMs. Hence, the authors decompose the task-solving into two steps: <em>thinking</em> in English and <em>extracting knowledge</em> multilingually from language-specific neurons in the middle layers of MLLMs.</p> </li> <li> <p>Generation of output - <br/>- <strong>MWork</strong> proposes that models <em>generate</em> responses by translating back to the query’s original language. <em>At this stage, one can think of a simple verifying experiment for this step</em>. Suppose we have a query in two non-English languages say, French and Urdu. Consider a query requiring extraction of a knowledge point prevalently common in both French and Urdu literature. <em>Will there be an inconsistency in MLLM performance?</em> (Maybe we can intercept the residual embeddings to see the checkpoint of back-translation initiation for languages structurally dissimilar to English.)</p> </li> </ol> <p><strong>How will I verify it?</strong></p> <p>I will look at each of the three steps separately: <em>conversion</em> to unified English representation, <em>task-solving</em> and <em>translation</em>. <strong><em>Conversion</em></strong> -&gt; Relative amount of English / non-English tokens; <em>Does the converted representation still relate to the original query? (Can we quantify it?) &lt;- Credits to Eshaan Tanwar from LCS2 for suggesting this.</em> 2. <strong><em>Task-Solving</em> MLWork</strong> claims that knowledge is extracted from language-specific neurons in the FFN structures. <em>Does deactivating these neurons affect the performance?</em> ALSO, is the knowledge diffused into different languages or localized to the language of the pretraining text? <em>Consider a setup when some knowledge is only limited to Vietnamese literature. Does deactivating neurons of FFN of Vietnamese completely downgrade MLLM performance?</em><br/><br/><br/> Is there a way to identify neurons relating to some knowledge?<br/> How much overlap do they have with language-specific neurons?<br/><br/><br/> <em><strong>Translation</strong></em> -&gt; I will certainly want to look at works trying to mechanistically interpret translation in MLLMs. (Is there a gap here?). Using those, one can design experiments to test this. <br/> The authors found that “<em>deactivating randomly sampled neurons in the task-solving layers disables the capabilities of LLMs in reasoning to a greater extent than deactivating randomly sampled neurons in all layers</em>”. This proves that the middle layers are associated with <em>thinking</em> + <em>extracting knowledge</em> (basically <em>task-solving</em>). (Note the authors refer to these <em>middle</em> layers of MLLMs as the <em>task-solving</em> layer.) <br/><br/> <strong>How to verify separation of <em>thinking in English</em> and <em>extracting knowledge multilingually?</em></strong> <br/><br/></p> <ul> <li>I would have created a synthetic task that does not involve any nuanced knowledge extraction (<em>Commonsense reasoning</em> over multiple languages). Taking some non-English languages under study : <strong>(1)</strong> Deactivate these language-specific neurons in the FFN structures of the <em>task-solving</em> layer. (Since I am not sure if knowledge from one language gets leaked into other language-specific neurons of FFN, let me propose all of these language-specific neurons of FFN) <strong>(2)</strong> Compare the performance of MLLMs with and without deactivation. Moreover, deactivating language-specific neurons of the attention structures in the <em>task-solving</em> layer should not have much effect on non-English queries as compared to English queries.</li> <li>The authors also claim that <em>deactivating language-specific neurons within the feed-forward structure of the task-solving layer predominantly affects non-English languages</em>. This implies that processing multilingual queries necessitates accessing the multilingual information embedded within the relevant structures.<br/><br/></li> </ul> <p><strong>How to verify the translation/generation structure of MWork</strong>- The authors have deactivated language-specific neurons in the final layers of the MLLMs and commented on a decline in performance. As I mentioned earlier, there can be ways to selectively comment on the nature of translation happening at this stage. The authors have further shown that selectively fine-tuning the parameters in the language-specific neurons enhances the performance of MLLMs for queries in that language. <br/> <strong>Questions for further research -</strong></p> <ul> <li>Polyglots are shown to process native languages at much greater ease as compared to other languages, even at the same level of proficiency. Given we declare it as the native language for the LLMs, can we quantify if being subjective to pre-training in multilingual corpora, affects the <em>ease</em> of handling English queries? (Let the setting be purely reasoning-based and let <em>ease</em> be quantified as the number of neurons activated in the task-solving layer.</li> </ul>]]></content><author><name></name></author><category term="papers"/><category term="paper"/><category term="multilingualism"/><summary type="html"><![CDATA[How do Large Language Models Handle Multilingualism?]]></summary></entry><entry><title type="html">a post with math</title><link href="https://prasoon1207.github.io/blog/2015/math/" rel="alternate" type="text/html" title="a post with math"/><published>2015-10-20T15:12:00+00:00</published><updated>2015-10-20T15:12:00+00:00</updated><id>https://prasoon1207.github.io/blog/2015/math</id><content type="html" xml:base="https://prasoon1207.github.io/blog/2015/math/"><![CDATA[<div style="margin-left: 30px; margin-right: 30px;"> In this work, the authors introduce a new framework for explaining the property of multilingualism in Multilingual Large Language Models (MLLMs). How do we define this property of 'Multilingualism'? The property of multilingualism enables MLLMs to text in multiple languages. In this paper, the authors have introduced a framework they call **MWork** which states that MLLMs first convert queries to a unified representation. They reason on this unified representation with multilingual knowledge extraction and finally, they translate queries to the target language. 1. Conversion to a _unified representation_ - <br/> The authors state that the first step in handling non-English queries is to convert them to English in the initial layers of MLLMs. According to Hou et al., 2023, attention mechanisms are utilized in reasoning over a text corpus. The authors observe a decrease in the number of _non-English_ neurons (?) of the attention matrices in the middle layers of the MLLMs. _Can reasoning be performed over text in a language other than English?_ It is unlikely owing to the decline in the number of non-English neurons in the reasoning (attention structures) regions of MLLMs. 2. Task-Solving - <br/> _What about the neurons in the FFN layers?_ - The authors observe NO decrease in the number of language-specific neurons in FFN structures across all layers of MLLMs. Hence, the authors decompose the task-solving into two steps: _thinking_ in English and _extracting knowledge_ multilingually from language-specific neurons in the middle layers of MLLMs. 3. Generation of output - <br/>- **MWork** proposes that models _generate_ responses by translating back to the query's original language. _At this stage, one can think of a simple verifying experiment for this step_. Suppose we have a query in two non-English languages say, French and Urdu. Consider a query requiring extracting a knowledge point prevalently common in French and Urdu literature. _Will there be an inconsistency in MLLM performance?_ (Maybe we can intercept the residual embeddings to see the checkpoint of back-translation initiation for languages structurally dissimilar to English.) **How will I verify it?** I will look at each of the three steps separately: _conversion_ to unified English representation, _task-solving_ and _translation_. **_Conversion_** -&gt; Relative amount of English / non-English tokens; _Does the converted representation still relate to the original query? (Can we quantify it?) &lt;- Credits to Eshaan Tanwar from LCS2 for suggesting this._ 2. **_Task-Solving_ MLWork** claims that knowledge is extracted from language-specific neurons in the FFN structures. _Does deactivating these neurons affect the performance?_ ALSO, is the knowledge diffused into different languages or localized to the language of the pretraining text? _Consider a setup when some knowledge is only limited to Vietnamese literature. Does deactivating neurons of FFN of Vietnamese completely downgrade MLLM performance?_<br/><br/><br/> Is there a way to identify neurons relating to some knowledge?<br/> How much overlap do they have with language-specific neurons?<br/><br/><br/> _**Translation**_ -&gt; I will certainly want to look at works trying to mechanistically interpret translation in MLLMs. (Is there a gap here?). Using those, one can design experiments to test this. <br/> The authors found that "_deactivating randomly sampled neurons in the task-solving layers disables the capabilities of LLMs in reasoning to a greater extent than deactivating randomly sampled neurons in all layers_". This proves that the middle layers are associated with _thinking_ + _extracting knowledge_ (basically _task-solving_). (Note the authors refer to these _middle_ layers of MLLMs as the _task-solving_ layer.) <br/><br/> **How to verify separation of _thinking in English_ and _extracting knowledge multilingually?_** <br/><br/> - I would have created a synthetic task that does not involve any nuanced knowledge extraction (_Commonsense reasoning_ over multiple languages). Taking some non-English languages under study : **(1)** Deactivate these language-specific neurons in the FFN structures of the _task-solving_ layer. (Since I am not sure if knowledge from one language gets leaked into other language-specific neurons of FFN, let me propose all of these language-specific neurons of FFN) **(2)** Compare the performance of MLLMs with and without deactivation. Moreover, deactivating language-specific neurons of the attention structures in the _task-solving_ layer should not have much effect on non-English queries as compared to English queries. - The authors also claim that _deactivating language-specific neurons within the feed-forward structure of the task-solving layer predominantly affects non-English languages_. This implies that processing multilingual queries necessitates accessing the multilingual information embedded within the relevant structures.<br/><br/> **How to verify the translation/generation structure of MWork**- The authors have deactivated language-specific neurons in the final layers of the MLLMs and commented on a decline in performance. As I mentioned earlier, there can be ways to selectively comment on the nature of translation happening at this stage. The authors have further shown that selectively fine-tuning the parameters in the language-specific neurons enhances the performance of MLLMs for queries in that language. <br/> **Questions for further research -** - Polyglots are shown to process native languages at much greater ease as compared to other languages, even at the same level of proficiency. Given we declare it as the native language for the LLMs, can we quantify if being subjective to pre-training in multilingual corpora, affects the _ease_ of handling English queries? (Let the setting be purely reasoning-based and let _ease_ be quantified as the number of neurons activated in the task-solving layer. </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[an example of a blog post with some math]]></summary></entry></feed>