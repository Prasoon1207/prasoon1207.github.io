---
---

@misc{hengle2024multilingualneedlehaystackinvestigating,
      title={Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models}, 
      author={Amey Hengle and Prasoon Bajpai and Soham Dan and Tanmoy Chakraborty},
      year={2024},
      eprint={2408.10151},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.10151}, 
      selected={true},
      bibtex_show={true},
      abstract={While recent large language models (LLMs) demonstrate remarkable abilities in responding to queries in diverse languages, their ability to handle long multilingual contexts is unexplored. As such, a systematic evaluation of the long-context capabilities of LLMs in multilingual settings is crucial, specifically in the context of information retrieval. To address this gap, we introduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to assess a model's ability to retrieve relevant information (the needle) from a collection of multilingual distractor texts (the haystack). This test serves as an extension of the multilingual question-answering task, encompassing both monolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMs on MLNeedle. Our findings reveal that model performance can vary significantly with language and needle position. Specifically, we observe that model performance is the lowest when the needle is (i) in a language outside the English language family and (ii) located in the middle of the input context. Furthermore, although some models claim a context size of 8k tokens or greater, none demonstrate satisfactory cross-lingual retrieval performance as the context length increases. Our analysis provides key insights into the long-context behavior of LLMs in multilingual settings to guide future evaluation protocols. To our knowledge, this is the first study to investigate the multilingual long-context behavior of LLMs.},
      code={https://github.com/AmeyHengle/multilingual-needle-in-a-haystack}
      
}

@article{bajpai2024llm_science_miscommunication ,
  title={Can LLMs Replace Neil DeGrasse Tyson: Evaluating Reliability of LLMs as Science Communicators},
  author={Prasoon Bajpai and Niladri Chatterjee and Subhabrata Dutta and Tanmoy Chakraborty},
  year={2024},
  eprint={2409.14037},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url = {https://arxiv.org/abs/2409.14037},
  journal={arXiv preprint},
  year={2024},
  selected = {true},
  bibtex_show = {true},
}

@misc{bajpai2024informationanxietylargelanguage,
      title={Information Anxiety in Large Language Models}, 
      author={Prasoon Bajpai and Sarah Masud and Tanmoy Chakraborty},
      year={2024},
      eprint={2411.10813},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.10813}, 
      abstract = {Large Language Models (LLMs) have demonstrated strong performance as knowledge repositories, enabling models to understand user queries and generate accurate and context-aware responses. Extensive evaluation setups have corroborated the positive correlation between the retrieval capability of LLMs and the frequency of entities in their pretraining corpus. We take the investigation further by conducting a comprehensive analysis of the internal reasoning and retrieval mechanisms of LLMs. Our work focuses on three critical dimensions - the impact of entity popularity, the models' sensitivity to lexical variations in query formulation, and the progression of hidden state representations across LLM layers. Our preliminary findings reveal that popular questions facilitate early convergence of internal states toward the correct answer. However, as the popularity of a query increases, retrieved attributes across lexical variations become increasingly dissimilar and less accurate. Interestingly, we find that LLMs struggle to disentangle facts, grounded in distinct relations, from their parametric memory when dealing with highly popular subjects. Through a case study, we explore these latent strains within LLMs when processing highly popular queries, a phenomenon we term information anxiety. The emergence of information anxiety in LLMs underscores the adversarial injection in the form of linguistic variations and calls for a more holistic evaluation of frequently occurring entities.},
      bibtex_show={true}
}
